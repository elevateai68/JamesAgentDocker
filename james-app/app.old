# file: james-app/app.py
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, UploadFile, File, Form
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn, os, platform, psutil, subprocess, httpx, json

# --- Load the persona from the file with the CORRECT filename ---
SYSTEM_PROMPT = "You are a helpful assistant." # A simple fallback prompt
try:
    with open("james_personality.txt", "r", encoding="utf-8") as f:
        SYSTEM_PROMPT = f.read().strip()
    print("INFO:     James persona loaded from file.")
except FileNotFoundError:
    print("WARNING:  james_personality.txt not found. Using default system prompt.")
# --- END ---

app = FastAPI()
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/", response_class=FileResponse)
def serve_index():
    return FileResponse("static/index.html")

class SystemInfo(BaseModel): cpu: str; ram_gb: float; gpu: str; active_model: str
@app.get("/system", response_model=SystemInfo)
def get_system_info():
    cpu = platform.processor() or platform.machine()
    ram = round(psutil.virtual_memory().total / 1e9, 2)
    try: am = subprocess.check_output("ollama list", shell=True, text=True).splitlines()[1]
    except: am = "None"
    return SystemInfo(cpu=cpu, ram_gb=ram, gpu="Unknown", active_model=am)

OLLAMA_API = os.getenv("OLLAMA_API_BASE", "http://ollama:11434/v1")

@app.websocket("/ws")
async def ws(ws: WebSocket):
    await ws.accept()
    try:
        data = json.loads(await ws.receive_text())
        msgs = [{"role":"system","content":SYSTEM_PROMPT},
                {"role":"user","content":f"{data['prompt']} {data.get('context','')}"}]
        
        payload = {"model":"qwen2.5:7b", "messages":msgs, "stream": True}
        
        async with httpx.AsyncClient(timeout=120) as c:
            async with c.stream("POST", f"{OLLAMA_API}/chat/completions", json=payload) as r:
                r.raise_for_status()
                async for chunk in r.aiter_text():
                    if not chunk.strip():
                        continue
                    try:
                        if chunk.startswith("data: "):
                            chunk = chunk[len("data: "):]
                        
                        if chunk.strip() == "[DONE]":
                            continue
                            
                        pkt = json.loads(chunk)
                        
                        content = pkt.get("choices", [{}])[0].get("delta", {}).get("content")
                        if content:
                            await ws.send_text(content)
                    except json.JSONDecodeError:
                        print(f"Warning: Skipping non-JSON chunk: {chunk}")
                        continue
        await ws.send_text("[Done]")
    except WebSocketDisconnect:
        print("Client disconnected.")
    except Exception as e:
        error_message = f"[Error] {type(e).__name__}: {e}"
        print(error_message)
        await ws.send_text(error_message)

@app.post("/remember")
async def remember(what: str = Form(...), who: str = Form(...), file: UploadFile = File(None)):
    return {"status":"ok","what":what,"who":who,"filename": file.filename if file else None}

class ChatReq(BaseModel): message: str
@app.post("/chat")
async def chat(rq: ChatReq):
    msgs = [{"role":"system","content":SYSTEM_PROMPT},
            {"role":"user","content":rq.message}]
    async with httpx.AsyncClient(timeout=120) as c:
        try:
            resp = await c.post(f"{OLLAMA_API}/chat/completions", json={"model":"qwen2.5:7b","messages":msgs})
            resp.raise_for_status()
            d = resp.json()
            return {"response": d["choices"][0]["message"]["content"], "messages":[rq.message]}
        except Exception as e:
            return {"error": str(e)}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
